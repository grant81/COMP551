{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 Report <br> \n",
    "##### Yingnan Zhao id: 260563769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes in a file path that stores the data, it will read the data and return a np array contains the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CSVfileReader(filePath):\n",
    "    with open(filePath) as file:\n",
    "        readCSV = csv.reader(file, delimiter=',')\n",
    "        data = []\n",
    "        for row in readCSV:\n",
    "            inputTemp = []\n",
    "            for i in range(len(row)):\n",
    "                if(row[i]!= ''):\n",
    "                    inputTemp.append(float(row[i]))\n",
    "            data.append(inputTemp)\n",
    "    data = np.array(data,dtype='float')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will first generate two sets of data according to the provided mean vector and covariance matrix(2000 examples each), for each set of data an extra column will be added at the end, -1 represents class1 and 1 represents class 2. Then it will shuffle the data and pick 30% of each set to form the test set, and 70% of each class to form the training set. They are named DS1_test.csv and DS1_train.csv respectively and stored in the hwk2_datasets_corrected folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData1(covPath,meanPath1,meanPath2):\n",
    "    covData = CSVfileReader(covPath)\n",
    "    meanData1 = CSVfileReader(meanPath1)\n",
    "    meanData2 = CSVfileReader(meanPath2)\n",
    "    class1 = np.random.multivariate_normal(meanData1[0],covData,2000)\n",
    "    c1 = -1* np.ones((2000,21),dtype='float') #-1 when using u0, c1\n",
    "    c1[:,:-1] = class1\n",
    "    class2 = np.random.multivariate_normal(meanData2[0],covData,2000)\n",
    "    c2 = np.ones((2000,21),dtype='float') #1 when using u1, c2\n",
    "    c2[:,:-1]= class2\n",
    "    np.random.shuffle(c1)\n",
    "    np.random.shuffle(c2)\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for i in range(2000):\n",
    "        if (i<600):\n",
    "            testSet.append(c1[i])\n",
    "            testSet.append(c2[i])\n",
    "        else:\n",
    "            trainSet.append(c1[i])\n",
    "            trainSet.append(c2[i])\n",
    "    shuffle(testSet)\n",
    "    shuffle(trainSet)\n",
    "    with open('hwk2_datasets_corrected\\\\DS1_test.csv', 'w', newline='') as file:\n",
    "        csvWriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for j in range(len(testSet)):\n",
    "            csvWriter.writerow(testSet[j])\n",
    "    with open('hwk2_datasets_corrected\\\\DS1_train.csv', 'w', newline='') as file:\n",
    "        csvWriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for j in range(len(trainSet)):\n",
    "            csvWriter.writerow(trainSet[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will produce W0 and W1 from the training data, first by calculating the possibility and mean for each class and the covariance matrix. Finally we get w0 and w1 fromthe equation below.<br> <img src=\"LDAEquation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(trainingPath):\n",
    "    X = CSVfileReader(trainingPath)\n",
    "    sum1 = np.zeros(X[0].shape,dtype='float')\n",
    "    sum2 = np.zeros(X[0].shape,dtype='float')\n",
    "    N0 = 0\n",
    "    N1 = 0\n",
    "    for i in range(len(X)):\n",
    "        if (X[i][len(X[0])-1]==-1):\n",
    "            sum1 = np.add(sum1,X[i])\n",
    "            N0 +=1\n",
    "        elif (X[i][len(X[0])-1]==1):\n",
    "            sum2 = np.add(sum2,X[i])\n",
    "            N1 +=1\n",
    "    mean1 = np.divide(sum1,N0)\n",
    "    mean2 = np.divide(sum2,N1)\n",
    "    mean1 = mean1[:-1].reshape(1,-1)\n",
    "    mean2 = mean2[:-1].reshape(1,-1)\n",
    "    P1 = N0/len(X)\n",
    "    P2 = N1/len(X)\n",
    "    sum = np.zeros((len(mean1[0]),len(mean1[0])))\n",
    "    for i in range(len(X)):\n",
    "        step1 = np.subtract(X[i][:-1],mean1)\n",
    "        step2 = np.subtract(X[i][:-1],mean2)\n",
    "        step3 = np.dot(np.transpose(step1),step1)\n",
    "        step4 = np.dot(np.transpose(step2),step2)\n",
    "        sum = np.add(np.add(step3,step4),sum)\n",
    "\n",
    "    cov = np.divide(sum,N0+N1)\n",
    "    covInverse = np.linalg.inv(cov)\n",
    "    w1 = np.dot(covInverse,np.transpose(np.subtract(mean1,mean2)))\n",
    "    term1 = 1/2*np.dot(np.dot(mean1,covInverse),np.transpose(mean1))\n",
    "    term2 = 1/2*np.dot(np.dot(mean2,covInverse),np.transpose(mean2))\n",
    "    w0 = np.subtract(term2,term1) + np.log(P1)- np.log(P2)\n",
    "    w = [w0,w1]\n",
    "    print('parameter learnt')\n",
    "    print('w0 = '+str(w0[0][0]))\n",
    "    print('w1 = '+str(w1))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will calculate the use the LDA classifier by using w0 and w1 produced by last function and calculate the expected class of each data from a data set and compare with the correct class. The accruacy, precision, recall and F1 measure will be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testLDA (testFilePath,w):\n",
    "    X = CSVfileReader(testFilePath)\n",
    "    result = []\n",
    "    TPTN = 0.0\n",
    "    TPFP = 0\n",
    "    TNFN = 0\n",
    "    TP =0\n",
    "    TN =0\n",
    "    w0 = w[0]\n",
    "    w1 = w[1]\n",
    "    for i in range(len(X)):\n",
    "        ans = w0[0][0] + np.dot(X[i][:-1],w1)\n",
    "        if (ans>0):\n",
    "            result.append(-1)\n",
    "            if (result[i] == X[i][len(X[0]) - 1]):\n",
    "                TPTN += 1\n",
    "                TN +=1\n",
    "            TNFN +=1\n",
    "        else:\n",
    "            result.append(1)\n",
    "            if (result[i] == X[i][len(X[0]) - 1]):\n",
    "                TPTN += 1\n",
    "                TP +=1\n",
    "            TPFP +=1\n",
    "    accuracy = TPTN/(TPFP+TNFN)\n",
    "    precision = TP/TPFP\n",
    "    recall = TP/(TP+TNFN-TN)\n",
    "    print('treating class 2 as positive')\n",
    "    print('Accuracy = '+str(accuracy))\n",
    "    print('Precision = '+str(precision))\n",
    "    print('Recall = '+str(recall))\n",
    "    print('F1 Measure = '+ str(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test on data set one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter learnt\n",
      "w0 = 2.01239190359\n",
      "w1 = [[ 1.05511023]\n",
      " [-0.63276528]\n",
      " [-0.41290599]\n",
      " [-0.22683658]\n",
      " [-0.72655371]\n",
      " [-0.29721443]\n",
      " [ 1.24715225]\n",
      " [-1.78586254]\n",
      " [-2.14790584]\n",
      " [ 0.66975918]\n",
      " [-0.95714165]\n",
      " [-0.90663457]\n",
      " [ 1.15530344]\n",
      " [ 0.96745546]\n",
      " [-0.42961878]\n",
      " [ 0.97122599]\n",
      " [ 2.16524577]\n",
      " [-0.50713488]\n",
      " [-0.04769346]\n",
      " [-0.37327559]]\n",
      "treating class 2 as positive\n",
      "Accuracy = 0.955\n",
      "Precision = 0.9595959595959596\n",
      "Recall = 0.95\n",
      "F1 Measure = 0.9547738693467336\n"
     ]
    }
   ],
   "source": [
    "w = LDA('hwk2_datasets_corrected\\\\DS1_train.csv')\n",
    "testLDA('hwk2_datasets_corrected\\\\DS1_test.csv',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, for DS1 the LDA model yields decent result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will first calculate the distance between the test example and all the samples in the training dataset using the equation below. <br> <img src=\"KNNdistance.png\"> then it will find the k nearest neighbors and assign the input x to the class that is most common in the k neighbors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kNN (trainX,inputX,k):\n",
    "\n",
    "    distanceSet = []\n",
    "    for i in range (len(trainX)):\n",
    "        sum = 0\n",
    "        for j in range(len(inputX)-1):\n",
    "            sum += np.square(trainX[i][j]-inputX[j])\n",
    "        distanceSet.append([np.sqrt(sum),i,trainX[i][len(trainX[0])-1]])\n",
    "    distanceSet = sorted(distanceSet)\n",
    "    neighbors = []\n",
    "    for i in range (k):\n",
    "        neighbors.append(distanceSet[i][2])\n",
    "    result = np.sum(neighbors)\n",
    "    if (result>0):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will use the KNN classifier to produce the class for each data sample in the test set and compare the result with the actual class of that sample, it will find the k that yields the best F1 score.Before applying the classifier, the data is normalized by the sklearn preprocessing normalize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testKNN (trainingSet, testingSet):\n",
    "    X = CSVfileReader(trainingSet)\n",
    "    X[:, :-1] = preprocessing.normalize(X[:, :-1], axis=0)\n",
    "    XTest = CSVfileReader(testingSet)\n",
    "    XTest[:,:-1] = preprocessing.normalize(XTest[:,:-1],axis=0)\n",
    "    maxF1 = 0\n",
    "    maxK = 0\n",
    "    for j in range (1,21,2):\n",
    "        TPTN = 0.0\n",
    "        TPFP = 0\n",
    "        TNFN = 0\n",
    "        TP = 0\n",
    "        TN = 0\n",
    "        for i in range(len(XTest)):\n",
    "            ans = kNN(X,XTest[i],j)\n",
    "            if (ans>0):\n",
    "                if (ans == X[i][len(X[0]) - 1]):\n",
    "                    TPTN += 1\n",
    "                    TN +=1\n",
    "                TNFN +=1\n",
    "            else:\n",
    "                if (ans == X[i][len(X[0]) - 1]):\n",
    "                    TPTN += 1\n",
    "                    TP +=1\n",
    "                TPFP +=1\n",
    "        precision = TP / TPFP\n",
    "        recall = TP / (TP + TNFN - TN)\n",
    "        F1 = 2 * precision * recall / (precision + recall)\n",
    "        print('k = '+str(j))\n",
    "        print('F1 Measure = ' + str(F1))\n",
    "        if(F1 > maxF1):\n",
    "            maxF1 = F1\n",
    "            maxK = j\n",
    "    TPTN = 0.0\n",
    "    TPFP = 0\n",
    "    TNFN = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    #run again the best case\n",
    "    for i in range(len(XTest)):\n",
    "        ans = kNN(X, XTest[i], maxK)\n",
    "        if (ans > 0):\n",
    "            if (ans == X[i][len(X[0]) - 1]):\n",
    "                TPTN += 1\n",
    "                TN += 1\n",
    "            TNFN += 1\n",
    "        else:\n",
    "            if (ans == X[i][len(X[0]) - 1]):\n",
    "                TPTN += 1\n",
    "                TP += 1\n",
    "            TPFP += 1\n",
    "    accuracy = TPTN/(TPFP+TNFN)\n",
    "    precision = TP/TPFP\n",
    "    recall = TP/(TP+TNFN-TN)\n",
    "    print('treating class 2 as positive')\n",
    "    print('the Best K is ' + str(maxK))\n",
    "    print('Accuracy = '+str(accuracy))\n",
    "    print('Precision = '+str(precision))\n",
    "    print('Recall = '+str(recall))\n",
    "    print('F1 Measure = '+ str(2*precision*recall/(precision+recall)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test to find the best k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "F1 Measure = 0.48124428179322964\n",
      "k = 3\n",
      "F1 Measure = 0.4918032786885245\n",
      "k = 5\n",
      "F1 Measure = 0.4873477038425492\n",
      "k = 7\n",
      "F1 Measure = 0.4735376044568246\n",
      "k = 9\n",
      "F1 Measure = 0.4626865671641791\n",
      "k = 11\n",
      "F1 Measure = 0.46760563380281694\n",
      "k = 13\n",
      "F1 Measure = 0.4619718309859155\n",
      "k = 15\n",
      "F1 Measure = 0.47457627118644063\n",
      "k = 17\n",
      "F1 Measure = 0.4711447492904447\n",
      "k = 19\n",
      "F1 Measure = 0.47826086956521735\n",
      "treating class 2 as positive\n",
      "the Best K is 3\n",
      "Accuracy = 0.535\n",
      "Precision = 0.5113636363636364\n",
      "Recall = 0.47368421052631576\n",
      "F1 Measure = 0.4918032786885245\n"
     ]
    }
   ],
   "source": [
    "testKNN('hwk2_datasets_corrected\\\\DS1_train.csv','hwk2_datasets_corrected\\\\DS1_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best number of neighbor is 3, it achieves an accuracy of 53.5% which is slightly better than chance, the classifier is considerably worse than the LDA classifier. And the F1 measure varies slightly when using different k value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will first generate the single multivariate Gaussian distributed data for each class according to the input mean and coverance, 6 in total. Then it will randomly pick data from those class according to the probability given (0.1,0.42,0.48) to form the dataset for each of the two class. After that, it will take 30% of the data from each class to form the test set and 70% to form the train set. They are named DS2_test.csv and DS2_train.csv respectively and stored in the hwk2_datasets_corrected folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generateData2(mean_11,mean_12,mean_13,mean_21,mean_22,mean_23,cov_1,cov_2,cov_3):\n",
    "    mean11 = CSVfileReader(mean_11)\n",
    "    mean12 = CSVfileReader(mean_12)\n",
    "    mean13 = CSVfileReader(mean_13)\n",
    "    mean21 = CSVfileReader(mean_21)\n",
    "    mean22 = CSVfileReader(mean_22)\n",
    "    mean23 = CSVfileReader(mean_23)\n",
    "    cov1 = CSVfileReader(cov_1)\n",
    "    cov2 = CSVfileReader(cov_2)\n",
    "    cov3 = CSVfileReader(cov_3)\n",
    "    class11 = np.random.multivariate_normal(mean11[0], cov1, 2000)\n",
    "    class12 = np.random.multivariate_normal(mean12[0], cov2, 2000)\n",
    "    class13 = np.random.multivariate_normal(mean13[0], cov3, 2000)\n",
    "    class21 = np.random.multivariate_normal(mean21[0], cov1, 2000)\n",
    "    class22 = np.random.multivariate_normal(mean22[0], cov2, 2000)\n",
    "    class23 = np.random.multivariate_normal(mean23[0], cov3, 2000)\n",
    "    np.random.shuffle(class11)\n",
    "    np.random.shuffle(class12)\n",
    "    np.random.shuffle(class13)\n",
    "    np.random.shuffle(class21)\n",
    "    np.random.shuffle(class22)\n",
    "    np.random.shuffle(class23)\n",
    "    c1 = []\n",
    "    c2 = []\n",
    "    counter1 =0\n",
    "    counter2 = 0\n",
    "    counter3 =0\n",
    "    for i in range(2000):\n",
    "        choice = np.random.choice([1,2,3],1,p=[0.1,0.42,0.48])\n",
    "        if (choice == 1):\n",
    "            c1.append(class11[i])\n",
    "            c2.append(class21[i])\n",
    "            counter1+=1\n",
    "\n",
    "        elif (choice == 2):\n",
    "            c1.append(class12[i])\n",
    "            c2.append(class22[i])\n",
    "            counter2+=1\n",
    "        elif (choice == 3):\n",
    "            c1.append(class13[i])\n",
    "            c2.append(class23[i])\n",
    "            counter3+=1\n",
    "    dataPool1 = -1 * np.ones((2000, 21), dtype='float')  # -1 when using c1\n",
    "    dataPool2 = np.ones((2000, 21), dtype='float')  # 1 when using c2\n",
    "    dataPool1[:, :-1] = c1\n",
    "    dataPool2[:, :-1] = c2\n",
    "    np.random.shuffle(dataPool1)\n",
    "    np.random.shuffle(dataPool2)\n",
    "    testSet = []\n",
    "    trainSet = []\n",
    "    for i in range(2000):\n",
    "        if (i<600):\n",
    "            testSet.append(dataPool1[i])\n",
    "            testSet.append(dataPool2[i])\n",
    "        else:\n",
    "            trainSet.append(dataPool1[i])\n",
    "            trainSet.append(dataPool2[i])\n",
    "    shuffle(testSet)\n",
    "    shuffle(trainSet)\n",
    "    with open('hwk2_datasets_corrected\\\\DS2_test.csv', 'w', newline='') as file:\n",
    "        csvWriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for j in range(len(testSet)):\n",
    "            csvWriter.writerow(testSet[j])\n",
    "    with open('hwk2_datasets_corrected\\\\DS2_train.csv', 'w', newline='') as file:\n",
    "        csvWriter = csv.writer(file, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        for j in range(len(trainSet)):\n",
    "            csvWriter.writerow(trainSet[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the LDA classifier on the dataset 2 produced from question4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter learnt\n",
      "w0 = 0.0303414354516\n",
      "w1 = [[ 0.02985879]\n",
      " [-0.02545177]\n",
      " [ 0.00645555]\n",
      " [ 0.00826951]\n",
      " [-0.00987854]\n",
      " [ 0.00140088]\n",
      " [-0.00376674]\n",
      " [-0.02966947]\n",
      " [-0.02279957]\n",
      " [ 0.01878309]\n",
      " [ 0.01552167]\n",
      " [-0.0064863 ]\n",
      " [-0.04418118]\n",
      " [-0.010366  ]\n",
      " [ 0.01322158]\n",
      " [ 0.00740697]\n",
      " [ 0.0111111 ]\n",
      " [-0.00113356]\n",
      " [ 0.02137036]\n",
      " [-0.0070229 ]]\n",
      "treating class 2 as positive\n",
      "Accuracy = 0.52\n",
      "Precision = 0.520066889632107\n",
      "Recall = 0.5183333333333333\n",
      "F1 Measure = 0.5191986644407345\n"
     ]
    }
   ],
   "source": [
    "w = LDA('hwk2_datasets_corrected\\\\DS2_train.csv')\n",
    "testLDA('hwk2_datasets_corrected\\\\DS2_test.csv',w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above the all of the measures of LDA droped dramatically on dataset 2 which is a mixture of three different Gaussians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use the KNN classifier on the dataset 2 produced from question4. And try to find the optimal K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "F1 Measure = 0.5154471544715448\n",
      "k = 3\n",
      "F1 Measure = 0.5070892410341952\n",
      "k = 5\n",
      "F1 Measure = 0.5063938618925832\n",
      "k = 7\n",
      "F1 Measure = 0.5134899912967799\n",
      "k = 9\n",
      "F1 Measure = 0.5127304653204565\n",
      "k = 11\n",
      "F1 Measure = 0.506896551724138\n",
      "k = 13\n",
      "F1 Measure = 0.49826989619377166\n",
      "k = 15\n",
      "F1 Measure = 0.5047701647875109\n",
      "k = 17\n",
      "F1 Measure = 0.49387040280210165\n",
      "k = 19\n",
      "F1 Measure = 0.5\n",
      "treating class 2 as positive\n",
      "the Best K is 1\n",
      "Accuracy = 0.5033333333333333\n",
      "Precision = 0.5\n",
      "Recall = 0.5318791946308725\n",
      "F1 Measure = 0.5154471544715448\n"
     ]
    }
   ],
   "source": [
    "testKNN('hwk2_datasets_corrected\\\\DS2_train.csv','hwk2_datasets_corrected\\\\DS2_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the optimal k is 1, and it yields similar result as when the KNN classifier being applied to dataset 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a huge drop in all measures when applying the LDA classifier to the dataset 2, because dataset 2 is a mixture of 3 gaussian distributions that has different means and different covariance matrix. Thus the dataset violates the key assumptions of LDA classifier, the data is not a gaussian, it is a gaussian mixture, and not all data in each class share the same covariance matrix. However, for the KNN classifier, it does not have any explicit assumptions about the dataset. Thus, the performance measures of the KNN classifier remain relatively stable."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
